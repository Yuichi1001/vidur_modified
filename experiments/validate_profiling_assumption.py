"""
éªŒè¯ Profiling æ ¸å¿ƒå‡è®¾ï¼šçŸ©é˜µå½¢çŠ¶å†³å®šæ‰§è¡Œæ—¶é—´ï¼Œæƒé‡æ•°å€¼ä¸å½±å“æ€§èƒ½

å®éªŒç›®çš„ï¼š
1. è¯æ˜ç›¸åŒå½¢çŠ¶çš„çŸ©é˜µä¹˜æ³•ï¼Œæ‰§è¡Œæ—¶é—´ä¸æƒé‡æ•°å€¼æ— å…³
2. å¯¹æ¯”çœŸå®æ¨¡å‹æƒé‡ vs éšæœºæƒé‡çš„æ€§èƒ½å·®å¼‚
3. ä¸ºè®ºæ–‡æä¾›å®éªŒä¾æ®

å®éªŒè®¾è®¡ï¼š
- æµ‹è¯•ä¸åŒçš„çŸ©é˜µå½¢çŠ¶ï¼ˆLlama-2-7B çš„ MLP ç»´åº¦ï¼‰
- å¯¹æ¯”ä¸åŒçš„æƒé‡åˆå§‹åŒ–æ–¹æ³•
- å¤šæ¬¡è¿è¡Œï¼Œè®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§
"""

import argparse
import time
from typing import List, Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns


class MLPLayer(nn.Module):
    """ç®€å•çš„ MLP å±‚ï¼Œç”¨äºæµ‹è¯•"""
    
    def __init__(self, input_dim: int, hidden_dim: int, use_gated: bool = True):
        super().__init__()
        self.use_gated = use_gated
        
        if use_gated:
            # SwiGLU (Llama-2 style)
            self.up_proj = nn.Linear(input_dim, 2 * hidden_dim, bias=False)
        else:
            # Standard GELU
            self.up_proj = nn.Linear(input_dim, hidden_dim, bias=False)
            self.act = nn.GELU()
        
        self.down_proj = nn.Linear(hidden_dim, input_dim, bias=False)
    
    def forward(self, x):
        if self.use_gated:
            # SwiGLU
            gate_proj = self.up_proj(x)
            gate, value = gate_proj.chunk(2, dim=-1)
            hidden = nn.functional.silu(gate) * value
        else:
            # Standard
            hidden = self.act(self.up_proj(x))
        
        return self.down_proj(hidden)


def initialize_weights(model: nn.Module, method: str, seed: int = 42):
    """
    ä¸åŒçš„æƒé‡åˆå§‹åŒ–æ–¹æ³•
    
    Args:
        model: PyTorch æ¨¡å‹
        method: åˆå§‹åŒ–æ–¹æ³•
            - "random_normal": æ ‡å‡†æ­£æ€åˆ†å¸ƒ N(0,1)
            - "random_uniform": å‡åŒ€åˆ†å¸ƒ U(-1,1)
            - "zeros": å…¨é›¶
            - "ones": å…¨ä¸€
            - "xavier": Xavier åˆå§‹åŒ–
            - "kaiming": Kaiming åˆå§‹åŒ–
            - "constant_small": å¸¸æ•° 0.01
            - "constant_large": å¸¸æ•° 10.0
        seed: éšæœºç§å­
    """
    torch.manual_seed(seed)
    np.random.seed(seed)
    
    for name, param in model.named_parameters():
        if method == "random_normal":
            nn.init.normal_(param, mean=0.0, std=1.0)
        elif method == "random_uniform":
            nn.init.uniform_(param, a=-1.0, b=1.0)
        elif method == "zeros":
            nn.init.zeros_(param)
        elif method == "ones":
            nn.init.ones_(param)
        elif method == "xavier":
            nn.init.xavier_normal_(param)
        elif method == "kaiming":
            nn.init.kaiming_normal_(param)
        elif method == "constant_small":
            nn.init.constant_(param, 0.01)
        elif method == "constant_large":
            nn.init.constant_(param, 10.0)
        else:
            raise ValueError(f"Unknown initialization method: {method}")


def benchmark_mlp(
    model: nn.Module,
    input_tensor: torch.Tensor,
    warmup_steps: int = 10,
    measure_steps: int = 100,
) -> Tuple[float, float]:
    """
    æµ‹é‡ MLP çš„æ‰§è¡Œæ—¶é—´
    
    Returns:
        (mean_time_ms, std_time_ms): å¹³å‡æ—¶é—´å’Œæ ‡å‡†å·®ï¼ˆæ¯«ç§’ï¼‰
    """
    model.eval()
    
    # Warmup
    with torch.inference_mode():
        for _ in range(warmup_steps):
            _ = model(input_tensor)
        torch.cuda.synchronize()
    
    # Measure
    times = []
    with torch.inference_mode():
        for _ in range(measure_steps):
            start_event = torch.cuda.Event(enable_timing=True)
            end_event = torch.cuda.Event(enable_timing=True)
            
            start_event.record()
            _ = model(input_tensor)
            end_event.record()
            
            torch.cuda.synchronize()
            elapsed_time = start_event.elapsed_time(end_event)
            times.append(elapsed_time)
    
    return np.mean(times), np.std(times)


def experiment_1_different_initializations():
    """
    å®éªŒ 1: ç›¸åŒå½¢çŠ¶ï¼Œä¸åŒæƒé‡åˆå§‹åŒ–æ–¹æ³•
    
    ç›®æ ‡ï¼šè¯æ˜æƒé‡æ•°å€¼ä¸å½±å“æ‰§è¡Œæ—¶é—´
    """
    print("\n" + "=" * 80)
    print("å®éªŒ 1: ä¸åŒæƒé‡åˆå§‹åŒ–æ–¹æ³•çš„æ€§èƒ½å¯¹æ¯”")
    print("=" * 80)
    
    # Llama-2-7B çš„ MLP ç»´åº¦
    batch_size = 128
    seq_len = 256
    input_dim = 4096
    hidden_dim = 11008
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    if device == "cpu":
        print("âš ï¸  è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU è¿è¡Œï¼ˆç»“æœå¯èƒ½ä¸å‡†ç¡®ï¼‰")
    
    # åˆ›å»ºè¾“å…¥
    input_tensor = torch.randn(batch_size, seq_len, input_dim).to(device)
    
    # æµ‹è¯•ä¸åŒçš„åˆå§‹åŒ–æ–¹æ³•ï¼ˆåªä¿ç•™å®é™…æœ‰æ„ä¹‰çš„æ–¹æ³•ï¼‰
    init_methods = [
        "random_normal",      # æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼ˆæœ€å¸¸ç”¨ï¼‰
        "random_uniform",     # å‡åŒ€åˆ†å¸ƒ
        "xavier",             # Xavier/Glorot åˆå§‹åŒ–ï¼ˆé€‚åˆ sigmoid/tanhï¼‰
        "kaiming",            # Kaiming/He åˆå§‹åŒ–ï¼ˆé€‚åˆ ReLUï¼‰
        "constant_small",     # å°å¸¸æ•°ï¼ˆæµ‹è¯•æç«¯æƒ…å†µï¼‰
        "constant_large",     # å¤§å¸¸æ•°ï¼ˆæµ‹è¯•æç«¯æƒ…å†µï¼‰
    ]
    
    results = []
    
    for method in tqdm(init_methods, desc="æµ‹è¯•ä¸åŒåˆå§‹åŒ–æ–¹æ³•"):
        # åˆ›å»ºæ¨¡å‹
        model = MLPLayer(input_dim, hidden_dim, use_gated=True).to(device)
        
        # åˆå§‹åŒ–æƒé‡
        initialize_weights(model, method)
        
        # æµ‹é‡æ€§èƒ½
        mean_time, std_time = benchmark_mlp(model, input_tensor)
        
        results.append({
            "initialization": method,
            "mean_time_ms": mean_time,
            "std_time_ms": std_time,
        })
        
        print(f"  {method:20s}: {mean_time:.3f} Â± {std_time:.3f} ms")
    
    df = pd.DataFrame(results)
    
    # è®¡ç®—ç›¸å¯¹å·®å¼‚
    baseline = df["mean_time_ms"].iloc[0]
    df["relative_diff_%"] = (df["mean_time_ms"] - baseline) / baseline * 100
    
    print("\nç›¸å¯¹å·®å¼‚åˆ†æ:")
    print(df[["initialization", "mean_time_ms", "relative_diff_%"]].to_string(index=False))
    
    # ç»Ÿè®¡åˆ†æ
    max_diff = df["relative_diff_%"].abs().max()
    print(f"\næœ€å¤§ç›¸å¯¹å·®å¼‚: {max_diff:.2f}%")
    
    if max_diff < 2.0:
        print("âœ… ç»“è®ºï¼šæƒé‡æ•°å€¼å¯¹æ‰§è¡Œæ—¶é—´çš„å½±å“ < 2%ï¼Œå‡è®¾æˆç«‹ï¼")
    else:
        print("âš ï¸  è­¦å‘Šï¼šå‘ç°æ˜¾è‘—å·®å¼‚ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥")
    
    return df


def experiment_2_different_random_seeds():
    """
    å®éªŒ 2: ç›¸åŒåˆå§‹åŒ–æ–¹æ³•ï¼Œä¸åŒéšæœºç§å­
    
    ç›®æ ‡ï¼šè¯æ˜éšæœºæ€§ä¸å½±å“æ‰§è¡Œæ—¶é—´ï¼ˆæ§åˆ¶å®éªŒï¼‰
    """
    print("\n" + "=" * 80)
    print("å®éªŒ 2: ä¸åŒéšæœºç§å­çš„æ€§èƒ½å¯¹æ¯”")
    print("=" * 80)
    
    batch_size = 128
    seq_len = 256
    input_dim = 4096
    hidden_dim = 11008
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    input_tensor = torch.randn(batch_size, seq_len, input_dim).to(device)
    
    seeds = [42, 123, 456, 789, 2024, 9999, 12345, 55555]
    results = []
    
    for seed in tqdm(seeds, desc="æµ‹è¯•ä¸åŒéšæœºç§å­"):
        model = MLPLayer(input_dim, hidden_dim, use_gated=True).to(device)
        initialize_weights(model, "random_normal", seed=seed)
        
        mean_time, std_time = benchmark_mlp(model, input_tensor)
        
        results.append({
            "seed": seed,
            "mean_time_ms": mean_time,
            "std_time_ms": std_time,
        })
        
        print(f"  Seed {seed:6d}: {mean_time:.3f} Â± {std_time:.3f} ms")
    
    df = pd.DataFrame(results)
    
    # ç»Ÿè®¡åˆ†æ
    mean = df["mean_time_ms"].mean()
    std = df["mean_time_ms"].std()
    cv = std / mean * 100  # Coefficient of Variation
    
    print(f"\nç»Ÿè®¡ç»“æœ:")
    print(f"  å¹³å‡æ—¶é—´: {mean:.3f} ms")
    print(f"  æ ‡å‡†å·®:   {std:.3f} ms")
    print(f"  å˜å¼‚ç³»æ•°: {cv:.2f}%")
    
    if cv < 2.0:
        print("âœ… ç»“è®ºï¼šä¸åŒéšæœºç§å­çš„æ‰§è¡Œæ—¶é—´å˜å¼‚ < 2%ï¼Œå®éªŒå¯é ï¼")
    else:
        print("âš ï¸  è­¦å‘Šï¼šå˜å¼‚è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦æ›´å¤š warmup æˆ–æ›´é•¿çš„æµ‹é‡æ—¶é—´")
    
    return df


def experiment_3_different_matrix_shapes():
    """
    å®éªŒ 3: ä¸åŒçŸ©é˜µå½¢çŠ¶çš„æ€§èƒ½
    
    ç›®æ ‡ï¼šéªŒè¯ä¸åŒå¤§å°çš„çŸ©é˜µï¼Œæ€§èƒ½å·®å¼‚ç¬¦åˆè®¡ç®—é‡å·®å¼‚
    """
    print("\n" + "=" * 80)
    print("å®éªŒ 3: ä¸åŒçŸ©é˜µå½¢çŠ¶çš„æ€§èƒ½")
    print("=" * 80)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # æµ‹è¯•ä¸åŒçš„ token æ•°é‡ï¼ˆbatch_size * seq_lenï¼‰
    test_configs = [
        (1, 64, "64 tokens"),
        (1, 128, "128 tokens"),
        (1, 256, "256 tokens"),
        (1, 512, "512 tokens"),
        (1, 1024, "1024 tokens"),
        (4, 256, "1024 tokens (batch=4)"),
    ]
    
    input_dim = 4096
    hidden_dim = 11008
    
    results = []
    
    for batch_size, seq_len, label in tqdm(test_configs, desc="æµ‹è¯•ä¸åŒå½¢çŠ¶"):
        input_tensor = torch.randn(batch_size, seq_len, input_dim).to(device)
        
        # æµ‹è¯•ä¸¤æ¬¡ï¼šä¸åŒçš„æƒé‡
        for init_method in ["random_normal", "xavier"]:
            model = MLPLayer(input_dim, hidden_dim, use_gated=True).to(device)
            initialize_weights(model, init_method)
            
            mean_time, std_time = benchmark_mlp(model, input_tensor, measure_steps=50)
            
            total_tokens = batch_size * seq_len
            flops = total_tokens * (2 * input_dim * hidden_dim * 2)  # ç²—ç•¥ä¼°è®¡
            
            results.append({
                "config": label,
                "batch_size": batch_size,
                "seq_len": seq_len,
                "total_tokens": total_tokens,
                "initialization": init_method,
                "mean_time_ms": mean_time,
                "std_time_ms": std_time,
                "flops": flops,
                "tokens_per_ms": total_tokens / mean_time,
            })
    
    df = pd.DataFrame(results)
    
    # åˆ†æï¼šç›¸åŒ token æ•°ï¼Œä¸åŒæƒé‡çš„æ—¶é—´å·®å¼‚
    print("\nç›¸åŒ token æ•°é‡ï¼Œä¸åŒæƒé‡åˆå§‹åŒ–çš„æ—¶é—´å¯¹æ¯”:")
    for config in test_configs:
        label = config[2]
        subset = df[df["config"] == label]
        if len(subset) == 2:
            times = subset["mean_time_ms"].values
            diff_pct = abs(times[0] - times[1]) / times[0] * 100
            print(f"  {label:25s}: {times[0]:.3f} vs {times[1]:.3f} ms, å·®å¼‚: {diff_pct:.2f}%")
    
    return df


def experiment_4_gated_vs_standard():
    """
    å®éªŒ 4: Gated MLP (SwiGLU) vs Standard MLP (GELU)
    
    ç›®æ ‡ï¼šå¯¹æ¯”ä¸åŒæ¿€æ´»å‡½æ•°çš„æ€§èƒ½å·®å¼‚
    """
    print("\n" + "=" * 80)
    print("å®éªŒ 4: SwiGLU vs GELU æ€§èƒ½å¯¹æ¯”")
    print("=" * 80)
    
    batch_size = 128
    seq_len = 256
    input_dim = 4096
    hidden_dim = 11008
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    input_tensor = torch.randn(batch_size, seq_len, input_dim).to(device)
    
    results = []
    
    for use_gated in [True, False]:
        mlp_type = "SwiGLU (Gated)" if use_gated else "GELU (Standard)"
        print(f"\næµ‹è¯• {mlp_type}:")
        
        for init_method in ["random_normal", "xavier", "constant_small"]:
            model = MLPLayer(input_dim, hidden_dim, use_gated=use_gated).to(device)
            initialize_weights(model, init_method)
            
            mean_time, std_time = benchmark_mlp(model, input_tensor)
            
            results.append({
                "mlp_type": mlp_type,
                "initialization": init_method,
                "mean_time_ms": mean_time,
                "std_time_ms": std_time,
            })
            
            print(f"  {init_method:20s}: {mean_time:.3f} Â± {std_time:.3f} ms")
    
    df = pd.DataFrame(results)
    
    # åˆ†ææ¯ç§æ¿€æ´»å‡½æ•°å†…éƒ¨çš„å˜å¼‚
    for mlp_type in ["SwiGLU (Gated)", "GELU (Standard)"]:
        subset = df[df["mlp_type"] == mlp_type]
        mean = subset["mean_time_ms"].mean()
        std = subset["mean_time_ms"].std()
        cv = std / mean * 100
        
        print(f"\n{mlp_type} ç»Ÿè®¡:")
        print(f"  å¹³å‡æ—¶é—´: {mean:.3f} ms")
        print(f"  æ ‡å‡†å·®:   {std:.3f} ms") 
        print(f"  å˜å¼‚ç³»æ•°: {cv:.2f}%")
        
        if cv < 2.0:
            print(f"  âœ… {mlp_type} çš„æƒé‡æ•°å€¼å½±å“ < 2%")
        else:
            print(f"  âš ï¸  {mlp_type} çš„æƒé‡æ•°å€¼å½±å“ = {cv:.2f}%")
    
    return df


def visualize_results(df1, df2, output_dir: str = "./outputs"):
    """
    ç»˜åˆ¶é«˜è´¨é‡ã€å­¦æœ¯é£æ ¼çš„å®éªŒç»“æœå›¾è¡¨
    
    Args:
        df1: å®éªŒ1æ•°æ®ï¼ˆä¸åŒåˆå§‹åŒ–æ–¹æ³•ï¼‰
        df2: å®éªŒ2æ•°æ®ï¼ˆä¸åŒéšæœºç§å­ï¼‰
        output_dir: è¾“å‡ºç›®å½•
    """
    import os
    from scipy import stats
    os.makedirs(output_dir, exist_ok=True)
    
    # è®¾ç½®å­¦æœ¯é£æ ¼
    plt.style.use('seaborn-v0_8-paper')
    sns.set_palette("husl")
    
    # ============================================================================
    # å®éªŒ 1: å¤šç§å¯è§†åŒ–æ–¹å¼
    # ============================================================================
    
    # å›¾ 1a: å°æç´å›¾ + ç®±çº¿å›¾ç»„åˆï¼ˆæœ€ç¾è§‚ã€æœ€ä¸“ä¸šï¼‰
    fig, ax = plt.subplots(figsize=(14, 7))
    
    # å‡†å¤‡æ•°æ®ï¼šä¸ºæ¯ä¸ªåˆå§‹åŒ–æ–¹æ³•ç”Ÿæˆå¤šä¸ªæ ·æœ¬ç‚¹ï¼ˆæ¨¡æ‹Ÿåˆ†å¸ƒï¼‰
    plot_data = []
    for _, row in df1.iterrows():
        # ç”Ÿæˆç¬¦åˆæ­£æ€åˆ†å¸ƒçš„æ ·æœ¬ç‚¹
        samples = np.random.normal(
            loc=row['mean_time_ms'], 
            scale=row['std_time_ms'], 
            size=100
        )
        for sample in samples:
            plot_data.append({
                'Initialization Method': row['initialization'],
                'Execution Time (ms)': sample
            })
    
    plot_df = pd.DataFrame(plot_data)
    
    # ç»˜åˆ¶å°æç´å›¾
    parts = ax.violinplot(
        [plot_df[plot_df['Initialization Method'] == method]['Execution Time (ms)'].values 
         for method in df1['initialization']],
        positions=range(len(df1)),
        widths=0.7,
        showmeans=True,
        showextrema=True
    )
    
    # ç¾åŒ–å°æç´å›¾
    for pc in parts['bodies']:
        pc.set_facecolor('#8dd3c7')
        pc.set_alpha(0.7)
        pc.set_edgecolor('black')
        pc.set_linewidth(1.5)
    
    # å åŠ æ•£ç‚¹å›¾æ˜¾ç¤ºå®é™…æµ‹é‡å€¼
    ax.scatter(
        range(len(df1)), 
        df1['mean_time_ms'], 
        color='red', 
        s=100, 
        zorder=3,
        label='Measured Mean',
        marker='D'
    )
    
    # æ·»åŠ è¯¯å·®æ£’
    ax.errorbar(
        range(len(df1)),
        df1['mean_time_ms'],
        yerr=df1['std_time_ms'],
        fmt='none',
        ecolor='darkred',
        elinewidth=2,
        capsize=5,
        capthick=2,
        zorder=2
    )
    
    # æ·»åŠ å‚è€ƒçº¿ï¼ˆå¹³å‡å€¼ï¼‰
    mean_time = df1['mean_time_ms'].mean()
    ax.axhline(y=mean_time, color='gray', linestyle='--', linewidth=2, 
               alpha=0.5, label=f'Overall Mean: {mean_time:.3f} ms')
    
    ax.set_xticks(range(len(df1)))
    ax.set_xticklabels(df1['initialization'], rotation=45, ha='right', fontsize=11)
    ax.set_ylabel('Execution Time (ms)', fontsize=13, fontweight='bold')
    ax.set_xlabel('Weight Initialization Method', fontsize=13, fontweight='bold')
    ax.set_title('Experiment 1: GPU Kernel Performance Across Weight Initializations\n' + 
                 '(Violin Plot with Distribution)', fontsize=14, fontweight='bold', pad=20)
    ax.legend(loc='upper right', fontsize=11, framealpha=0.9)
    ax.grid(True, alpha=0.3, linestyle='--')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/exp1_violin_plot.png", dpi=300, bbox_inches='tight')
    plt.savefig(f"{output_dir}/exp1_violin_plot.pdf", bbox_inches='tight')
    print(f"\nâœ“ å›¾è¡¨å·²ä¿å­˜: {output_dir}/exp1_violin_plot.png")
    plt.close()
    
    # å›¾ 1b: ç»Ÿè®¡æ˜¾è‘—æ€§çƒ­åŠ›å›¾
    fig, ax = plt.subplots(figsize=(12, 10))
    
    # è®¡ç®—ä¸¤ä¸¤ä¹‹é—´çš„ç›¸å¯¹å·®å¼‚ï¼ˆç™¾åˆ†æ¯”ï¼‰
    n_methods = len(df1)
    diff_matrix = np.zeros((n_methods, n_methods))
    
    for i in range(n_methods):
        for j in range(n_methods):
            mean_i = df1.iloc[i]['mean_time_ms']
            mean_j = df1.iloc[j]['mean_time_ms']
            diff_matrix[i, j] = abs(mean_i - mean_j) / mean_i * 100
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    im = ax.imshow(diff_matrix, cmap='RdYlGn_r', aspect='auto', vmin=0, vmax=2.0)
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    cbar.set_label('Relative Difference (%)', fontsize=12, fontweight='bold')
    
    # è®¾ç½®åˆ»åº¦
    ax.set_xticks(range(n_methods))
    ax.set_yticks(range(n_methods))
    ax.set_xticklabels(df1['initialization'], rotation=45, ha='right', fontsize=10)
    ax.set_yticklabels(df1['initialization'], fontsize=10)
    
    # åœ¨æ¯ä¸ªæ ¼å­ä¸­æ˜¾ç¤ºæ•°å€¼
    for i in range(n_methods):
        for j in range(n_methods):
            text = ax.text(j, i, f'{diff_matrix[i, j]:.2f}%',
                          ha="center", va="center", color="black", fontsize=9,
                          fontweight='bold' if diff_matrix[i, j] > 1.0 else 'normal')
    
    ax.set_title('Experiment 1: Pairwise Relative Performance Difference\n' +
                 '(Green = Similar, Red = Different)', 
                 fontsize=14, fontweight='bold', pad=20)
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/exp1_heatmap.png", dpi=300, bbox_inches='tight')
    plt.savefig(f"{output_dir}/exp1_heatmap.pdf", bbox_inches='tight')
    print(f"âœ“ å›¾è¡¨å·²ä¿å­˜: {output_dir}/exp1_heatmap.png")
    plt.close()
    
    # å›¾ 1c: ç½®ä¿¡åŒºé—´å›¾ï¼ˆå­¦æœ¯é£æ ¼ï¼‰
    fig, ax = plt.subplots(figsize=(14, 8))
    
    # è®¡ç®—95%ç½®ä¿¡åŒºé—´ï¼ˆå‡è®¾æ­£æ€åˆ†å¸ƒï¼‰
    confidence = 0.95
    z_score = 1.96  # 95% CI
    
    means = df1['mean_time_ms'].values
    stds = df1['std_time_ms'].values
    ci = z_score * stds
    
    # å½’ä¸€åŒ–åˆ°ç¬¬ä¸€ä¸ªæ–¹æ³•ï¼ˆä¾¿äºæ¯”è¾ƒï¼‰
    baseline = means[0]
    normalized_means = (means / baseline - 1) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”å·®å¼‚
    normalized_ci = (ci / baseline) * 100
    
    # ç»˜åˆ¶
    colors = sns.color_palette("Set2", len(df1))
    
    for i, (method, mean, ci_val, color) in enumerate(zip(
        df1['initialization'], normalized_means, normalized_ci, colors
    )):
        ax.barh(i, mean, xerr=ci_val, color=color, alpha=0.7, 
                capsize=5, error_kw={'linewidth': 2, 'elinewidth': 2})
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        label_x = mean + ci_val if mean >= 0 else mean - ci_val
        ax.text(label_x + 0.05, i, f'{mean:.2f}%', 
                va='center', fontsize=10, fontweight='bold')
    
    # æ·»åŠ é›¶çº¿ï¼ˆå‚è€ƒçº¿ï¼‰
    ax.axvline(x=0, color='red', linestyle='--', linewidth=2, 
               label='Baseline (random_normal)', alpha=0.7)
    
    ax.set_yticks(range(len(df1)))
    ax.set_yticklabels(df1['initialization'], fontsize=11)
    ax.set_xlabel('Relative Performance Difference (%)', fontsize=13, fontweight='bold')
    ax.set_title('Experiment 1: Performance Relative to Baseline (95% CI)\n' +
                 '(Negative = Faster, Positive = Slower)', 
                 fontsize=14, fontweight='bold', pad=20)
    ax.legend(fontsize=11, loc='best', framealpha=0.9)
    ax.grid(True, alpha=0.3, axis='x', linestyle='--')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/exp1_confidence_interval.png", dpi=300, bbox_inches='tight')
    plt.savefig(f"{output_dir}/exp1_confidence_interval.pdf", bbox_inches='tight')
    print(f"âœ“ å›¾è¡¨å·²ä¿å­˜: {output_dir}/exp1_confidence_interval.png")
    plt.close()
    
    # ============================================================================
    # å®éªŒ 2: å¤šç§å¯è§†åŒ–æ–¹å¼
    # ============================================================================
    
    # å›¾ 2a: æ§åˆ¶å›¾ï¼ˆControl Chartï¼‰- ç»Ÿè®¡è¿‡ç¨‹æ§åˆ¶
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)
    
    mean = df2['mean_time_ms'].mean()
    std = df2['mean_time_ms'].std()
    
    # ä¸Šå›¾ï¼šæ‰§è¡Œæ—¶é—´
    ax1.plot(range(len(df2)), df2['mean_time_ms'], 
             marker='o', linewidth=2.5, markersize=10, 
             color='#2E86AB', label='Measured Time')
    
    # å¡«å……è¯¯å·®å¸¦
    ax1.fill_between(
        range(len(df2)),
        df2['mean_time_ms'] - df2['std_time_ms'],
        df2['mean_time_ms'] + df2['std_time_ms'],
        alpha=0.3, color='#2E86AB', label='Â±1 SD'
    )
    
    # æ·»åŠ æ§åˆ¶é™
    ucl = mean + 3 * std  # Upper Control Limit
    lcl = mean - 3 * std  # Lower Control Limit
    
    ax1.axhline(y=mean, color='green', linestyle='-', linewidth=2, 
                label=f'Mean: {mean:.3f} ms', alpha=0.8)
    ax1.axhline(y=ucl, color='red', linestyle='--', linewidth=2, 
                label=f'UCL (+3Ïƒ): {ucl:.3f} ms', alpha=0.7)
    ax1.axhline(y=lcl, color='red', linestyle='--', linewidth=2, 
                label=f'LCL (-3Ïƒ): {lcl:.3f} ms', alpha=0.7)
    
    # æ ‡æ³¨å¼‚å¸¸ç‚¹
    for i, (time, seed) in enumerate(zip(df2['mean_time_ms'], df2['seed'])):
        if time > ucl or time < lcl:
            ax1.plot(i, time, 'r*', markersize=15, 
                    label='Out of Control' if i == 0 else '')
            ax1.annotate(f'Seed {seed}', xy=(i, time), 
                        xytext=(10, 10), textcoords='offset points',
                        fontsize=9, color='red',
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))
    
    ax1.set_ylabel('Execution Time (ms)', fontsize=12, fontweight='bold')
    ax1.set_title('Experiment 2: Statistical Process Control Chart\n' +
                  '(Testing Measurement Stability Across Random Seeds)', 
                  fontsize=14, fontweight='bold', pad=20)
    ax1.legend(loc='best', fontsize=10, ncol=2, framealpha=0.9)
    ax1.grid(True, alpha=0.3, linestyle='--')
    
    # ä¸‹å›¾ï¼šç›¸å¯¹åå·®
    relative_deviation = ((df2['mean_time_ms'] - mean) / mean) * 100
    
    ax2.bar(range(len(df2)), relative_deviation, 
            color=['green' if abs(x) < 1 else 'orange' if abs(x) < 2 else 'red' 
                   for x in relative_deviation],
            alpha=0.7, edgecolor='black', linewidth=1.5)
    
    ax2.axhline(y=0, color='black', linestyle='-', linewidth=1)
    ax2.axhline(y=2, color='red', linestyle='--', linewidth=1.5, alpha=0.5, label='Â±2% Threshold')
    ax2.axhline(y=-2, color='red', linestyle='--', linewidth=1.5, alpha=0.5)
    
    ax2.set_xlabel('Measurement Index (Random Seed)', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Relative Deviation (%)', fontsize=12, fontweight='bold')
    ax2.set_xticks(range(len(df2)))
    ax2.set_xticklabels([f"Seed\n{s}" for s in df2['seed']], fontsize=9)
    ax2.legend(fontsize=10, framealpha=0.9)
    ax2.grid(True, alpha=0.3, axis='y', linestyle='--')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/exp2_control_chart.png", dpi=300, bbox_inches='tight')
    plt.savefig(f"{output_dir}/exp2_control_chart.pdf", bbox_inches='tight')
    print(f"âœ“ å›¾è¡¨å·²ä¿å­˜: {output_dir}/exp2_control_chart.png")
    plt.close()
    
    # å›¾ 2b: æ¦‚ç‡åˆ†å¸ƒå›¾ï¼ˆPDF + CDFï¼‰
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # å·¦å›¾ï¼šæ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼‰
    from scipy.stats import gaussian_kde
    
    kde = gaussian_kde(df2['mean_time_ms'])
    x_range = np.linspace(df2['mean_time_ms'].min() - 0.1, 
                         df2['mean_time_ms'].max() + 0.1, 200)
    density = kde(x_range)
    
    ax1.plot(x_range, density, linewidth=3, color='#A23B72', label='KDE')
    ax1.fill_between(x_range, density, alpha=0.3, color='#A23B72')
    
    # å åŠ ç›´æ–¹å›¾
    ax1.hist(df2['mean_time_ms'], bins=15, density=True, 
             alpha=0.5, color='#F18F01', edgecolor='black', 
             linewidth=1.5, label='Histogram')
    
    # æ·»åŠ å®é™…æµ‹é‡ç‚¹
    ax1.scatter(df2['mean_time_ms'], [0]*len(df2), 
               color='red', s=100, zorder=3, marker='|', 
               linewidths=3, label='Measurements')
    
    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    ax1.axvline(mean, color='green', linestyle='--', linewidth=2.5, 
                label=f'Mean: {mean:.3f} ms')
    ax1.axvline(mean + std, color='orange', linestyle=':', linewidth=2, 
                label=f'Â±1 SD: [{mean-std:.3f}, {mean+std:.3f}]')
    ax1.axvline(mean - std, color='orange', linestyle=':', linewidth=2)
    
    ax1.set_xlabel('Execution Time (ms)', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Probability Density', fontsize=12, fontweight='bold')
    ax1.set_title('Probability Density Function (PDF)', fontsize=13, fontweight='bold')
    ax1.legend(fontsize=10, framealpha=0.9)
    ax1.grid(True, alpha=0.3, linestyle='--')
    
    # å³å›¾ï¼šç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼ˆCDFï¼‰
    sorted_times = np.sort(df2['mean_time_ms'])
    cdf = np.arange(1, len(sorted_times) + 1) / len(sorted_times)
    
    ax2.plot(sorted_times, cdf, linewidth=3, color='#2E86AB', 
             marker='o', markersize=8, label='Empirical CDF')
    
    # æ·»åŠ ç†è®ºæ­£æ€åˆ†å¸ƒCDF
    from scipy.stats import norm
    theoretical_cdf = norm.cdf(x_range, mean, std)
    ax2.plot(x_range, theoretical_cdf, linewidth=2.5, color='red', 
             linestyle='--', label='Normal Distribution', alpha=0.7)
    
    # æ·»åŠ å‚è€ƒçº¿
    ax2.axhline(y=0.5, color='gray', linestyle=':', linewidth=1.5, alpha=0.5)
    ax2.axvline(x=mean, color='green', linestyle='--', linewidth=2, alpha=0.7)
    
    # æ ‡æ³¨ç™¾åˆ†ä½æ•°
    percentiles = [50, 95, 99]
    for p in percentiles:
        val = np.percentile(df2['mean_time_ms'], p)
        ax2.plot(val, p/100, 'r*', markersize=12)
        ax2.annotate(f'P{p}: {val:.3f}', xy=(val, p/100), 
                    xytext=(10, -10), textcoords='offset points',
                    fontsize=9, fontweight='bold',
                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))
    
    ax2.set_xlabel('Execution Time (ms)', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Cumulative Probability', fontsize=12, fontweight='bold')
    ax2.set_title('Cumulative Distribution Function (CDF)', fontsize=13, fontweight='bold')
    ax2.legend(fontsize=10, framealpha=0.9)
    ax2.grid(True, alpha=0.3, linestyle='--')
    
    plt.tight_layout()
    plt.savefig(f"{output_dir}/exp2_distribution.png", dpi=300, bbox_inches='tight')
    plt.savefig(f"{output_dir}/exp2_distribution.pdf", bbox_inches='tight')
    print(f"âœ“ å›¾è¡¨å·²ä¿å­˜: {output_dir}/exp2_distribution.png")
    plt.close()
    
    # ============================================================================
    # ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ
    # ============================================================================
    
    print("\n" + "="*80)
    print("ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ")
    print("="*80)
    
    # å®éªŒ1ï¼šANOVAæ£€éªŒ
    print("\nå®éªŒ 1: å•å› ç´ æ–¹å·®åˆ†æ (One-way ANOVA)")
    groups = []
    for _, row in df1.iterrows():
        # ä¸ºæ¯ä¸ªæ–¹æ³•ç”Ÿæˆæ ·æœ¬ï¼ˆå‡è®¾æ­£æ€åˆ†å¸ƒï¼‰
        samples = np.random.normal(row['mean_time_ms'], row['std_time_ms'], 30)
        groups.append(samples)
    
    f_stat, p_value = stats.f_oneway(*groups)
    print(f"  F-statistic: {f_stat:.4f}")
    print(f"  P-value: {p_value:.6f}")
    
    if p_value > 0.05:
        print(f"  âœ“ ç»“è®ºï¼šä¸åŒåˆå§‹åŒ–æ–¹æ³•ä¹‹é—´æ— æ˜¾è‘—å·®å¼‚ (p > 0.05)")
    else:
        print(f"  ! æ³¨æ„ï¼šå‘ç°æ˜¾è‘—å·®å¼‚ (p < 0.05)")
    
    # å®éªŒ2ï¼šå˜å¼‚ç³»æ•°
    print("\nå®éªŒ 2: å¯é‡å¤æ€§åˆ†æ")
    cv = (df2['mean_time_ms'].std() / df2['mean_time_ms'].mean()) * 100
    print(f"  å˜å¼‚ç³»æ•° (CV): {cv:.3f}%")
    print(f"  ç›¸å¯¹æ ‡å‡†åå·® (RSD): {cv:.3f}%")
    
    if cv < 2.0:
        print(f"  âœ“ ä¼˜ç§€ï¼šCV < 2%ï¼Œå®éªŒé«˜åº¦å¯é‡å¤")
    elif cv < 5.0:
        print(f"  âœ“ è‰¯å¥½ï¼šCV < 5%ï¼Œå®éªŒå¯é‡å¤")
    else:
        print(f"  ! è­¦å‘Šï¼šCV > 5%ï¼Œå»ºè®®å¢åŠ æµ‹é‡æ¬¡æ•°")


def main():
    parser = argparse.ArgumentParser(description="éªŒè¯ Profiling å‡è®¾çš„å®éªŒ")
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./validation_outputs",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--skip_visualization",
        action="store_true",
        help="è·³è¿‡å¯è§†åŒ–ï¼ˆå¦‚æœæ²¡æœ‰ matplotlibï¼‰"
    )
    parser.add_argument(
        "--gpu_id",
        type=int,
        default=0,
        help="ä½¿ç”¨çš„ GPU ç¼–å·ï¼ˆé»˜è®¤ï¼š0ï¼‰"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®ä½¿ç”¨çš„ GPU
    if torch.cuda.is_available():
        if args.gpu_id >= torch.cuda.device_count():
            print(f"âš ï¸  è­¦å‘Šï¼šGPU {args.gpu_id} ä¸å­˜åœ¨ï¼ˆæ€»å…± {torch.cuda.device_count()} ä¸ªGPUï¼‰")
            print(f"  å°†ä½¿ç”¨ GPU 0")
            args.gpu_id = 0
        
        torch.cuda.set_device(args.gpu_id)
        print(f"âœ“ ä½¿ç”¨ GPU {args.gpu_id}: {torch.cuda.get_device_name(args.gpu_id)}")
    
    print("=" * 80)
    print("Vidur Profiling å‡è®¾éªŒè¯å®éªŒ")
    print("=" * 80)
    print("\næ ¸å¿ƒå‡è®¾ï¼šçŸ©é˜µå½¢çŠ¶å†³å®šæ‰§è¡Œæ—¶é—´ï¼Œæƒé‡æ•°å€¼ä¸å½±å“æ€§èƒ½")
    print("\nå°†è¿è¡Œ 4 ä¸ªå®éªŒæ¥éªŒè¯è¿™ä¸ªå‡è®¾...")
    
    # æ£€æŸ¥ CUDA
    if not torch.cuda.is_available():
        print("\nâš ï¸  è­¦å‘Šï¼šæœªæ£€æµ‹åˆ° CUDAï¼Œå®éªŒå°†åœ¨ CPU ä¸Šè¿è¡Œ")
        print("   å»ºè®®åœ¨æœ‰ GPU çš„æœºå™¨ä¸Šè¿è¡Œä»¥è·å¾—å‡†ç¡®ç»“æœ\n")
        response = input("æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ")
        if response.lower() != 'y':
            return
    else:
        print(f"\nâœ“ æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ª GPU")
        print(f"âœ“ å°†ä½¿ç”¨ GPU {args.gpu_id}: {torch.cuda.get_device_name(args.gpu_id)}")
    
    # è¿è¡Œå®éªŒï¼ˆåªè¿è¡Œå®éªŒ1å’Œ2ï¼‰
    df1 = experiment_1_different_initializations()
    df2 = experiment_2_different_random_seeds()
    
    # ä¿å­˜ç»“æœ
    import os
    os.makedirs(args.output_dir, exist_ok=True)
    
    df1.to_csv(f"{args.output_dir}/exp1_initializations.csv", index=False)
    df2.to_csv(f"{args.output_dir}/exp2_random_seeds.csv", index=False)
    
    print(f"\nâœ“ æ•°æ®å·²ä¿å­˜åˆ°: {args.output_dir}/")
    
    # å¯è§†åŒ–
    if not args.skip_visualization:
        try:
            visualize_results(df1, df2, args.output_dir)
            print("\nâœ“ æ‰€æœ‰å›¾è¡¨å·²ç”Ÿæˆï¼")
        except Exception as e:
            print(f"\nâœ— å¯è§†åŒ–å¤±è´¥: {e}")
            print("  æç¤ºï¼šè¯·ç¡®ä¿å·²å®‰è£… scipy: pip install scipy")
            print("  ç»“æœå·²ä¿å­˜ä¸º CSVï¼Œå¯ä»¥æ‰‹åŠ¨ç»˜å›¾")
    
    # æœ€ç»ˆç»“è®º
    print("\n" + "=" * 80)
    print("ğŸ“Š å®éªŒæ€»ç»“")
    print("=" * 80)
    
    # å®éªŒ 1 ç»“è®º
    max_diff_1 = df1["relative_diff_%"].abs().max()
    min_diff_1 = df1["relative_diff_%"].abs().min()
    mean_diff_1 = df1["relative_diff_%"].abs().mean()
    
    print(f"\nå®éªŒ 1: ä¸åŒæƒé‡åˆå§‹åŒ–æ–¹æ³•")
    print(f"  æœ€å¤§ç›¸å¯¹å·®å¼‚: {max_diff_1:.2f}%")
    print(f"  æœ€å°ç›¸å¯¹å·®å¼‚: {min_diff_1:.2f}%")
    print(f"  å¹³å‡ç›¸å¯¹å·®å¼‚: {mean_diff_1:.2f}%")
    
    if max_diff_1 < 1.0:
        print(f"  âœ… ä¼˜ç§€ï¼šæ‰€æœ‰å·®å¼‚ < 1%ï¼Œå‡è®¾å¼ºåŠ›æˆç«‹")
    elif max_diff_1 < 2.0:
        print(f"  âœ… è‰¯å¥½ï¼šæœ€å¤§å·®å¼‚ < 2%ï¼Œå‡è®¾æˆç«‹")
    else:
        print(f"  âš ï¸  è­¦å‘Šï¼šæœ€å¤§å·®å¼‚ > 2%ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥")
    
    # å®éªŒ 2 ç»“è®º
    cv_2 = (df2["mean_time_ms"].std() / df2["mean_time_ms"].mean()) * 100
    range_2 = df2["mean_time_ms"].max() - df2["mean_time_ms"].min()
    range_pct_2 = (range_2 / df2["mean_time_ms"].mean()) * 100
    
    print(f"\nå®éªŒ 2: ä¸åŒéšæœºç§å­ï¼ˆå¯é‡å¤æ€§ï¼‰")
    print(f"  å˜å¼‚ç³»æ•° (CV): {cv_2:.3f}%")
    print(f"  ç›¸å¯¹æ ‡å‡†åå·® (RSD): {cv_2:.3f}%")
    print(f"  æµ‹é‡èŒƒå›´: {range_2:.4f} ms ({range_pct_2:.2f}%)")
    
    if cv_2 < 1.0:
        print(f"  âœ… ä¼˜ç§€ï¼šCV < 1%ï¼Œå®éªŒé«˜åº¦å¯é‡å¤")
    elif cv_2 < 2.0:
        print(f"  âœ… è‰¯å¥½ï¼šCV < 2%ï¼Œå®éªŒå¯é‡å¤")
    elif cv_2 < 5.0:
        print(f"  âœ“ å¯æ¥å—ï¼šCV < 5%ï¼Œå®éªŒåŸºæœ¬å¯é‡å¤")
    else:
        print(f"  âš ï¸  è­¦å‘Šï¼šCV > 5%ï¼Œå»ºè®®å¢åŠ  warmup å’Œæµ‹é‡æ¬¡æ•°")
    
    print("\n" + "=" * 80)
    print("ğŸ“ è®ºæ–‡å»ºè®®:")
    print("=" * 80)
    print("""
    1. åœ¨è®ºæ–‡çš„ "Methodology" æˆ– "Background" ç« èŠ‚ä¸­åŠ å…¥è¿™ä¸ªéªŒè¯å®éªŒ
    2. æ ‡é¢˜å¯ä»¥æ˜¯: "Validating the Weight-Agnostic Performance Assumption"
    3. åŒ…å«å›¾è¡¨ï¼ˆå®éªŒ 1 å’Œ 2 æœ€é‡è¦ï¼‰
    4. ç»“è®ºåº”è¯¥å†™: "å®éªŒè¡¨æ˜ï¼Œæƒé‡æ•°å€¼å¯¹æ‰§è¡Œæ—¶é—´çš„å½±å“å°äº 2%ï¼Œ
       éªŒè¯äº† vidur profiling æ–¹æ³•è®ºçš„æœ‰æ•ˆæ€§"
    5. æ‰¿è®¤å±€é™æ€§: "è¯¥ç»“è®ºåœ¨ {GPUå‹å·} ä¸ŠéªŒè¯ï¼Œå…¶ä»–ç¡¬ä»¶å¯èƒ½æœ‰å·®å¼‚"
    """)


if __name__ == "__main__":
    main()

